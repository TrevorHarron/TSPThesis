\documentclass[midd]{thesis}

\usepackage{graphicx}
\usepackage{times}
\usepackage{clrscode}
\usepackage{mathtools}

\newcommand{\tab}{\hspace*{2em}}

\bibliographystyle{plain}

\title{He Doesn't Know the Territory\\
\small{An investigation of Heuristics for the Traveling Salesman Problem.}}
\author{TREVOR HARRON}
\adviser{Professor Mathew Dickerson}

\parindent=0in
\parskip=7.2pt
\openup 1.5em

\begin{document}

\maketitle

\contentspage

\chapter{Introduction}
\tab There are several puzzles that haunt computer scientists in their quest for finding what can and cannot be computed with our current paradigms. Some of these problems have to do with graphs and answering questions about them. Some of these questions turned out to be easy to solve such as the Seven Bridges of Konigsberg where we want to go on each edge of a graph once. Other problems on the other hand are more difficult to solve such as finding Hamiltonian Cycles and arguably more importantly the Traveling Salesman Problem. The purpose of this paper is to examine the implementation of heuristics on real world data and examine not only their compleixty algorithmically but also in terms of implementation.\\
\tab The Traveling Salesman Problem (TSP) is simply the question if a salesman is trying to visit each city given a set of cities once without driving on the same road more than once and ending at his starting location again, what route does he take? In essence for a cyclical graph, what is the shortest Hamiltonian Cycle. A Hamiltonian Cycle is the route to visit each city once creating a circuit containing all of the cities. More formally the TSP is defined as given a Graph $G$ is a set of $N$ cities $C$ and a set of edges $E$ such that $e \in E = (c_{i}, c_{j},k)$ where $i \le N, j \le N$ and $i \ne j, c \in C,$ and with $k$ as the cost to travel on that path, find the path that covers every city with a minimal cost and returns to the starting city $c_{0}$ in $G$. While on the surface this seems like  a reasonable problem for a computer to solve it turns out to be quite complex to solve with a reasonable algorithm.\\
\tab The TSP turns out to be in a class of problems that computer scientists aren't sure if there is a efficient solution to the problem; these problems are known as NP-Hard. The algorithmic complexity of the naive solution where we check every possible route turns out to be $\Omega(n!)$. It is important to note that when I say an efficient solution I mean one that can be solved in a Polynomial amount of time (i.e. $O(n^k)$ where $k$ is a constant). Even for smaller instances of the TSP it turns out to be infeasible to use the naive solution for the case of trying to visit every capitol in the United States (starting at Washington D.C.) it would take an inordinate amount of time and processing capabilities to solve. This also means that for the graphs that are being examined the exact optimal distance cannot be known. Even with improvements to the algorithm such as using Linear Programming (which is outside the scope of this paper) the complexity is $O(n^{2}2^{n})$, which, while more efficient than the naive solution is still not polynomial making it  infeasible to always find the optimal solution. Despite this there is still the need for the TSP from companies such as FedEx and UPS for instance that depend on the quick and efficient delivery of mail and packages to a not fixed set of locations from day to day. In an effort to find other solutions to the TSP computer scientists work on efficient heuristic algorithms that may only find an approximate solution to the TSP. These heuristics are known as heuristics and are the subject of this paper. There are various permutations of the TSP that include the symmetric and the asymmetric TSP, but for the purposes of this paper there is not a distinction drawn between the two major sub-problems. Other forms of the TSP that are not part of this examination at all in this is the Euclidean TSP which does not use the network distances but instead the Euclidean distances (hence the name). Other instances that are not examined include the Constant TSP and the Small TSP, the Bottleneck TSP, Time-Dependent TSP, and the Stochastic TSP. There are some cases where the TSP has also been solved but those will not be examined either; in short this paper is trying to examine these heuristics in the most General form of the TSP possible with real world data.\\
\tab For this project the graph that these heuristics will be run on comes from the real set of cities from the continental United States and the edges of the graph are calculated based on the network distance (road distance) in between the cities. Given the impossibly large nature of the dataset, the graphs used are created from the cities and roads of a single state (with islands and other non-reachable cities removed) as opposed to using the continental United States as a whole. The overall problem that this paper intends to address is the implementations of various heuristic algorithms and showing that these heuristics can be used in real world scenarios and looking at the benefits and detractions of each heuristic. Finally after seeing this, the question then lies of what improvements could be made in terms of algorithmic space and time complexity and with the guaranteed upper bound as well as the complexity and overhead knowledge required for implementing a given heuristic.\\
\tab Most of the heuristics that are examined in this paper are also called Tour Construction heuristics. These are obviously heuristics that are used in the construction of the route and for the most part can guarantee being 2 times the optimal length. It is important to note that a subset of these heuristics are local search heuristics (such as Nearest Neighbor) and those cannot be guaranteed to be within a multiple of the optimal solution\cite{ttps}. Among these heuristics are the Nearest Neighbor heuristic, the Greedy heuristic, the Minimum Spanning Tree  heuristic, and the Christofides heuristic. The second heuristic that will be examined is a family of tour improvement heuristics based on Darwin's Theory of Evolution called Genetic Algorithms. Other Tour Optimization heuristics which focus on taking a given route and such as Simulated Annealing and the $k-opt$ family of tour optimization heuristics. Combinational approaches and Ant Colony approaches are not examined in the scope of this paper. Other approaches that aren't even considered in this paper are Approximation heuristics, Branch and Bound heuristics. In short the heuristics that are covered in this paper are only a small portion of the possible heuristics in the world of TSP heuristic studies. The reason for limiting the research is to allow for a more in depth examination.\\
\tab To test the heuristics, an Experimenter was made to go through all of the different datasets for each of the solvers multiple times. Of course the number of times that these experiments are is not a small number to eliminate any possibly for chance or circumstance interfering with the results. The results from the various heuristics is  the time in seconds required to solve the graph as well as the Megabytes needed for space along with the distance. For each solver, there will be several states used to create the various graphs needed for testing. These graphs, will vary in size of the set of cities as well as the number of routes from one city to the next. Each of these graphs with be used to see the overall effects that the size of the graph has on the performance of the heuristics.\\
\tab Before moving on to more details of the heuristics being studied, there is one important concept that should be kept in mind, which is the Held-Karp lower bound. The Held-Karp lower bound is a good measurement of comparison of an heuristics performance. For instance, there will be several references to how close to the Held-Karp Lower bound another heuristic can get to. As for how the Held-Karp lower bound is computed, this is out of the scope of this paper. It is sufficient to say however that the approach is ``...the solution to the linear programming relaxation of the integer representation of the TSP'' \cite{htspc}. In short, the Held Karp algorithm used to create the Held-Karp lower bound is a solution using a non-graphical representation of the TSP. This polynomial time algorithm has been found to be within .08\% of  the optimal length in most cases and at worst within $2/3$ of the optimal tour. As such is a reliable metric to use in evaluating heuristics of the TSP. In short it is important to keep this as a concept since it will be referenced several times as a means of algorithmic evaluation.\\
\tab As whole, heuristics have been studied as a potential alternative to the naive solution to the TSP. This pursuit then led to the a conclusion stating that if there was a heuristic that guaranteed an optimal solution there then $P = NP$. For the most  part, computer scientists agree that $P \neq NP$ and as such there is not an efficient solution that will also guarantee the optimal solution. In lieu of that, to investigate the various heuristics of the TSP, several heuristics have been chosen for their algorithmic complexity and guaranteed maximum distances as an example of most of the conventional approaches that currently exist. These heuristics are Nearest Neighbor, Greedy, Minimum panning Tree, Genetic, and Christofides heuristics. Each heuristic will be implemented and then tested based on their runtime, their CPU usage. After this, it will be explored if their exists a hybrid algorithm that can utilize the best attributes of the traditional heuristics or another approach combining the different kind of construction and path optimization approaches.\\
\tab The heuristic algorithms implemented will also be measured on several different qualities. These qualities include the algorithmic time and space complexity as well as the actual run times and CPU usage and on the guaranteed upper bound of the various heuristics. In addition to those measurements, the heuristics will also be evaluated on the complexity of implementation. The complexity of implementation means the number additional functions needed for the heuristic, the extra data structures aside from the graph as well and the additional knowledge that the implementer would need. Encompassing some of this complexity is also the additional work needed for the helper functions for the heuristics as well. These issues of complexity, though mostly qualitative in nature, will be examined in the most quantitative ways possible, in looking at the amount of time needed for writing each of these heuristics in addition to any comments about the number of lines of codes, the number of class variables, etc. These will then be put through t-tests to see if there are any significant differences in the results in terms of one heuristic's efficiency in the different qualities. These results in combination with the notes on implementation will then be used to see if there are significant improvements that could be made.\\
\tab Java is the language that will be used for this project. Java is it is an Object Oriented Programming language that uses a Virtual Machine to run its code. The reason for this is that this provides a layer of abstraction for the developer from the system s/he is working on. This allows for code that is written on one kind of system to be run on other kinds of systems as long as they all have a compatible version of Java (and thus the Java Virtual Machine or JVM) on them. The choice to use Java is mainly due to its platform independent nature for the implementation for each heuristic. In addition to Java the other languages considered for this project were NetLogo, Python, or C++. NetLogo is a simple multi-agent based language and as such is not suited to the flexible implementation of TSP heuristics or in the gathering of data needed to draw necessary conclusions. Python is a simple interpreted language that would be easy to implement the heuristics with but also can lack the durability of other languages as well as support and use in the workforce for large scale projects where efficiency of the heuristics becomes key.  While C++ might be easier to examine the memory usage and is also faster, it is dependent on the system that the heuristic that is running on. C++ also lacks some of the durability of Java making it less than ideal for larger problems and individual efforts such as this to create heuristics. As a result of these different factors, Java, while not the fastest language not only abstracts from the machine's system but also provides the durability of for a individual endeavor and for instances of large datasets at the cost of some difficulty in extracting memory usage. This difficulty is to be expected because of the fact that Java programs run on the JVM instead of the Computer itself. The reason this drawback is a fair trade-off is that it avoids any issues with questions regarding code dependency on one specific operating system or about questions of efficiency based on the system itself. Another reason for using Java is that several companies that depend on these heuristic algorithms also use Java. Part of the purpose of this project is to test the implementation capabilities and as such choosing Java was done with the fact that several organizations whose services require a fast an efficient solution to TSP would also use Java as their main development language. Java will also be  used to both parse the data files as well as the visualization of the graph and pathways. Each of the experiments will also be written and run Java naturally. For clarity in each heuristic will then  be displayed in both their pseudo-code format as well as their implementation in Java in addition to the  explanation. Having said all of this, the purpose of this paper is not to argue the implementation in one language over another but instead to focus on the implementation of heuristics for the TSP.\\
\tab The implementations are in Java, it is important to give a brief description of the basic lasses/interfaces used for such a project. The interface most concerned with the graph's construction (apart from the Graph interface) is the Reader interface who's job it is to parse information in either a Comma Separated Value (csv) document or a key markup language (kml) document. As a note: to be able to parse and use kml documents required to construct the graph an external library designed by the makers of the KML format (citation here) was used to read the of all of the cities was used. Readers can also be implemented to take any other number of formats in theory but for the purposes of this implementation kml files and csv files were focused on. The Reader is able to take the documents preprocessed from the shapefile format as described in Chapter 2 into either csv (if used from the GIS OD Cost Matrix) or the kml format preprocessed from Census city data. The different readers that are implemented from the Reader Interface are then used to parse these different formats: csv for roads, csv for locations, and kml for city locations.\\
\tab After the Reader  interface is more importantly the Graph interface. This interface allows for different representations of the graph to be used abstractly through some key methods. There are some basics that each graph has such as Nodes and Edges to represent the cities and roads. The two graph implementations used for this were the Matrix model and the list model. The matrix model is where ta graph a represented in an $n$x$n$ matrix, where $n$ is the number of cities, and the edge $c_i,c_j,k$ goes from city $c_i$ to city $c_j$ at a cost $k$ and is at position $(i,j)$ in the matrix such that $i \neq j$. Those edges that are on the diagonal have been declared $null$ for the purposes of simplifying the implementation. The list model has a slightly different structure where the Nodes contains a list of pairs of Nodes and distances from Node to Node. The reason for such a difference is that certain hueristics work on one graph implementation than another. These two different kinds of graphs also represent the two major ways that directed graphs are represented mathematically. It is also important to note that due to some of the restrictions of programming languages (and for the sake of simplicity in parsing the data) The Edges of the graph are directed though the for each city pair there is an edge going from one city to the other and visa-versa (this happens to be the case in the instances presented in this paper). In essence, this makes abstracts away from the complicated potential of changing the heuristics for directed roads and non-directed and allows for us to better examine the heuristics themselves.\\
\tab For the Edge and Node classes they are relatively straight forward in implementation. For the Edge class (there is only one) the edge holds the object identifiers for both the city the road is going from as well as the city the road is going to and the distance from the two points. As stated earlier, this distance is not Euclidean but is instead based off of the network distance traveled by road to get from one place to the other. The final point to make in regards to the Edge class is that it implements the Comparable interface (from the Java standard library) allowing it to be sorted in the case where the sorting of Edges is needed. The Node super-class is very simple in that basically it holds the basic information about the city/point of interest such as the coordinate x and y, the name of the city, the unique identifier of the location (generally a number) and the position it was read in. The last variable is to allow for the Node to know where it lies in say a matrix based graph. There are also two sub-classes of the Node class which are Matrix Node and List Node. This implementation is similar except that the List Node keeps track of the roads that it is attached too. This is to allow for easier implementation with various tree based heuristics or ones that focus on the Edges more than the nodes.\\  
\tab The final and arguably most important interface to mention is the Solver. The basic idea of the solver interface is that the classes that implement the solver interface are the ones that implement the heuristics tested in the paper. Each solver deals with the implementation of the heuristics. These are all created by a factory to help better abstract away from the direct implementations of the solvers. This is used by both the Experimenter class (used for the running of experiments) and for the TSPApplet which is the GUI for looking at how the solvers are solving the TSP for the various heuristics. for each different heuristic, there is a solver that goes with it. Also for purposes of clarity in the Java code, the helper functions of the heuristics are not displayed as that would clutter the examination of the algorithms (except in certain cases where they are the defining factor of the algorithm's time and or space). Each solver also contains the necessary container classes to solve the problem and methods that go along with that; for example a Minimum Spanning Tree Solver (or MSTSolver) would contain a Tree object and the method for transversing the tree. These more specific cases will be gone into more detail in the Algorithms section of this paper. For purposes of clarity, the result of the solver is an ArrayList (re sizable array) of the cycle of cities, the distance of that cycle as well and the time, and space used to solve the graph. The For each solver, testing has been done to ensure the accuracy of the heuristic's implementation, the accuracy of the solution on a smaller (less than 10 cities) dataset as well as the durability of the code in place. This dataset has no bearing on an place in reality and as such is not considered in the analysis of the heuristic algorithms.\\
\tab The final set of classes that should be discussed are the ones pertaining to the visualization and the experimentation of the project. These classes are pretty self explanatory and focus on using the Readers to create the graph and the Solvers to solve it. In the terms of the differences between the Experimenter and the TSPApplet, the Experimenter finds the solution and repeats the experiment a certain number of times adding the results to a csv file to be analyzed with t-tests or to use for graphic representations of the results. The TSPApplet is a Java applet whose purpose it is to view the results of a single iteration of the the solver for a given graph. The graph and the solver can both be chosen by the user to allow for easy viewing of the heuristics of the TSP. Overall the need for these classes while not strictly what is being examined are paramount to the study, analysis and debugging of the various heuristics implemented in the course of this paper.\\
\tab As a final note pertaining the the code used within this paper, it was all also written with good coding practices in mind. As a result of this, the code developed was tested incrementally on small data sets as mentioned before and tried to use elements of good Object Oriented design. As for the exact specifications of good Object Oriented Design, those are outside the scope of this paper. However, in all stages of development, code was written to be flexible and reusable and attempted to minimize the amount of rewriting that can go on in a project such as this. As such all of the classes are in packages appropriate to their function, names either are intuitive or were named to mimic the pseudo-code (for readability in side by side comparisons), inheritance was used as a means of preventing rewriting and Interfaces were used in order to abstract away from implementation dependencies. In short, the code developed in the course of this paper was made to try and match industry standards given that there is a strong undertone usability of various heuristics in a space where a trade off must be made in between efficiency and accuracy in the real world.\\
\tab Going forward there are a few things key concepts and terms that will be the core of this paper. The first of these is that the TSP is a difficult problem to solve and as such the heuristic algorithms used to approximate the solution are the main focus of study. The various heuristics are implemented in Java as well as the testing, visual, and other necessary parts to creating the graph. The data (which will be explained in more detail) is based off of real world data and as such the coding practices used focus on real world implementing of these heuristics. In lieu of this the focus of this paper lies not only in the algorithmic time and space but also in the complexity of the implementation as well.

\chapter{The Data and Methodologies}
\section{The Data}
\tab The data used for this project was a combination of road census data for the edges of the graphs and their costs. The road and city data are from 2006 and as such it should be noted that the main use of this data is to show some of the practical applications of the algorithms that are implemented in the course of of this paper. The scope of this data will be examined as well as the contents and implications that this may have on the paper as a whole. While this is not critical to the algorithms or the overall results of the paper it is important for the data to be examined in detail to better to understand the scope of the paper as the whole.\\
\tab The cities (or vertexes) are also from US census data containing the major towns and cities in each of the 48 continental states.  Each of these files were originally in a format called a shapefile. Shapefiles are used by GIS software and contains the geometric data for the objects within the shapefile. These various features include an unique identification number, the geometric shape of the objects, relevant information regarding the geometry, the coordinates of the object, its name, and other pertinent features. While shapefiles are useful for GIS studies the format is not parsable for the purposes of this paper. Before going into the preprocessing of the data first the details of the data should be examined for both the cities and the roads that will be used for this project.\\
\tab For the instance of the cities, the geometry was simply a point with no additional data attached to it apart from the coordinates for the point, the cities also have a name and a state that they are in, the basic information including the city's id and also the additional information that might be useful for GIS investigation including population size, range, and the county that the city is a part of. However, lacking from the data is the road or roads or any mention of the roads that, the city is closest to. Most importantly, it should be noted that the cities are by no means a comprehensive list of all of the populated places within the United States though admittedly is it a larger set than the average layman would think of.\\
\tab The roads are made up of several different segments and the junctions that they connect to. These junctions and lengths are also held are small vectors that make up an arc. It is important to note that these are not exact and in some cases required some additional preprocessing that will be described later. However, this property of the road while approximate in nature is still reasonably as accurate as one could hope. Also, the additional soon to be further explained preprocessing changes did not change in any way the underlying distance data of the lines. For the distance data that each segment has, for the purposes of easy comparison and accuracy the use of meters was chosen. This is not a choice based of the practicality of the use of meters in the United States but instead the practicality of the metric system.\\
\tab The basic notion of the data is that the there are two different kinds of raw data that are used for this paper. The two kinds of data are based of the cities and the roads. Both of these are originally in a shapefile to be read for a set of GIS programs which this paper does snot implement. Despite this the benefit of this format is the additional data that is involved. Though while most of this meta-data is not useful for the purposes of this paper there still are some pieces that are vital to the paper including the geometric data in particular. It is important to finally note that the data that has been up until now as a whole is truly two separate sets of data. The two sets are based on the two different kinds of objects required for the paper: cities and roads. The combining and binding of these two different sets of data happens in the data preprocessing stage.
\section{Data Preprocessing}
\tab To gather the data, US Census data from 2006 was used to for the cities and roads in the US and pre processed using ArcGIS. The set of cities was saved from its original shapefile format to a readable format called kml (Key Markup Languages) which maintained all of the spatial and geometric properties of the original format. These properties include the state that the city resides in, the coordinates of the city, and additional properties that a user might useful. These features are the ones that are in the shapefile previously mentioned. To transfer the shapefiles into the more readable kml and csv file formats a tool was used called ogr2ogr (pronounced `'Oger to Oger""). Ogr2ogr is a open source tool whose sole purpose is to `'translate" shapefiles to other formats. These formats range from other GIS formats like raster, to a variety of json called geojson, kml, csv and more. Among these formats there was research done as to what the best format to use for both the cities and the roads before chosing the final formats to store and parse the data in.\\
\tab Before even considering translating the formats into a parsable format the data needs to be processed from its raw form into the graphical form that is used. Then to calculate the network distances of each city, ArcGIS was used. To gather the distances first in ArcGIS is that an OD Cost Matrix was created from the cities of a given state and all of the roads from the census data. Naturally, to ensure that there are not any errors that occur either from either the preprocessing that ArcGIS does in laying down roadways on the map there was some manual labor involved in ensuring the roads that are supposed to connect to each other do. Such errors can occur due to slight miscalculations in the data itself or in how the shapefiles are processed in ArcGIS. The corrections that occurred include the connection of roads that are continuous, the linking of cities to the closest road, and even telling the OD Cost matrix that  some roads end at the coastline away from cities and should not be considered. In the creation of the OD Cost Matrix the cities are filtered by the state that is being used in the creation of the graph for the vertexes. All of these edges are placed as the origins and the destinations of the matrix to try and find all of the edges for the graph that are possible. To ensure the completeness of the graph and that there are not any disconnected cities, the cities that are on unreachable locations such as islands have also been removed. Such features would also make several of the algorithms infeasible.\\
\tab The Reader class is primarily responsible for parsing the various formats and converting it for the uses within the Graph. In terms of the implementations, the classes for reading the kml and the csv formats are distinctive to what the data is used for. For instance the KMLReader reads the kml file of cities and uses the meta-data to create the cities for the graph to hold. With the CSVReader, the initial implementation meant that the roads were the only part to be read but this could easily be changed to accept arbitrary locations of interest to visit as well as the edges. To ensure that the algorithms and parsing work correctly, there is also a sample dataset that have been created consisting of five vertexes which are interconnected by edges with costs. This ample data uses the same formats as the actual data.\\
\tab For the distances and edges, a variant of Dijkstra's algorithm is used\cite{gis}. These distances were then stored in line objects that consists of an id, a pair of cities in the form of `'to - from" and ending in the distance of the route. All of these line objects were then stored in a shapefile tied to the OD Cost Matrix. To read these lines the shapefile takes from export it and then translate it into a csv file. In the case of a csv file there is not the geometry of the lines at all. This lack of geometry is not of concern however since the lines are tied to the cities meaning that the geometry of the lines is not needed at all. In fact the only line geometry that is regarded is the lines that are stored in the roads which has been mentioned before. For the lines that are then represented in the graphics are represented as a straight line instead of the network distances. This is purely for the ease of displaying the graph as a whole.\\ 
\section{Methods}
\tab The methods used for testing are very simple in terms of concepts needed. The information that is being gathered is simply the amount of time needed to run the algorithm, the space, and the distance of the route that the algorithm produces. To gather all of this extra data, it is simplest to gather it when obtaining the result, which is simply an array of the order of cities are visited in to get the route. Of course also to gather this information there is extra code that has been added to the algorithms which have no effect on the result itself nor its time or space usage in any significant way and of course have no impact on the algorithmic evaluations.\\
\tab To gather the time that it took a given heuristic to run, Java has some built in features to help keep track of the time an opteration was started which is the basis for the time keeping that is used. Admittedly, the fully time keeper is gathering the starting time which Java keeps as a long then immediately after the program is complete finding the current time (also a long), find the difference between the two, and then converting it into a readable format. The amount of time is recorded in the final solution in terms of seconds. For the smaller datasets concern is understandable for the purposes of measuring time some might find the use of tenths of seconds more useful but for the larger cities examined, it would soon become clear that seconds is not only valid but easily puts the results in context without meaningless conversion. The amount of time that it takes the program to run is the first of measurements to be added to the result.\\
\tab As stated in the introduction, since Java is used as the language of implementation, gathering memory usage is not a straight-forward matter. However, to be able to look at the memory usage, all that is simply done is use a function that calls Java's garbage collector and then look at the total memory and free memory to see how much was used. The function called is `'System.gc()'  for `Garbage Collection'. This technique is frequently used in programs to check for memory leaks (i.e. data that is not cleaned up properly. The calls to get the used amount of memory and the total memory are System.totalMemory() and System.freeMemory(). By finding the difference in the two we can find the amount of used memory there was. However, also as mentioned earlier, the memory is being collected in terms of KB so the difference is divided by 1024 (since 1024 bytes go into a KB). To avoid any issues this might have on the time, the space is taken after the time is taken. This number of KB that is used is also then added to the result that is returned.\\
\tab Distance is the final quantitative measure that the heuristics are being evaluated on. The distances themselves come from the road distances that were calculated as stated earlier in Data Preprocessing. The gathering of distance occurs at different times depending on the heuristic in question but are either gathered during the construction of the route or after the route has been constructed. For the gathering of the distances during the tour's construction, the distance is only aggregated after it has been confirmed that a route from one city to another has already been added to the final tour produced. Also, for each of the tours it is needless to say that the final distance back to the original starting city has been included in the final distance. Also all the distances are kept in their original form of meters for easy base 10 conversions for later calculations of the results. The distance is the final measurement added to the results.\\
\tab To evaluate the code and the heuristics overall there are two main forms of evaluation, quantitative and qualitative. It is simple enough to understand the quantitative measures, these are based on the use of time and space and are measured in terms of big-O notation. In addition to the algorithmic time and space the actual time and space and distance result of the algorithm are also measured. Slightly in between these two approaches is the evaluation of the difficulty of the implementation itself. The quantitative approach for evaluation is the number of extra helper functions and the algorithmic complexity and implementation of these functions. The qualitative approach to the heuristics comes from looking at the knowledge required to implement these algorithms. This is a qualitative measure given that this information while measurable in terms of number of concepts, does not lend itself to more definite measures. More of the qualitative measures also include the intuitive difficulty of the heuristics in both understanding the approach itself as well as the intuition of the implementation. The quantitative measures of the algorithms are then confirmed as different ranging from heuristics to heuristic with t-tests to show if there are significant differences between the results of the algorithms. In conclusion, the methods for evaluation are both quantitative and qualitative and for the msot part, there are many attempts of quantify all of the data.

\chapter{Algorithms}
\section{Conventions of Pseudo-code}
\tab The pseudo-code used in this paper does not conform to a single form of pseudo-code but instead attempts to be as readable as possible. Some points to keep note of include the use of plain English and mathematical symbols to try and concisely give the algorithm in a readable format. To augment this endeavor some plainly named, black box functions have also been used. The pseudo-code also appears in a more Object Oriented Programing approach to better link the understanding between the pseudo-code and the Java code.
\section{Nearest Neighbor}
\tab One of the simplest heuristic algorithms is Nearest Neighbor (or NN for short). This algorithm's is to first start at a random city (or a specified city) look locally at the cities nearby and then adds the closest unvisited city to its to the solution. Then second keep adding cities in this method until there are no cities left and then finally find and add the edge back to the starting city. This is obviously a local and greedy approach to solving the TSP. It is to be noted that while in some cases the cost of the algorithm is taken after the route has been found but for this implementation of nearest neighbor the distance of the route is taken when the city is added to the route. The basic pseudo-code for the algorithm briefly described above is shown below:\\
$\proc{NNHeuristic}(G)$:
\begin{codebox}
\li $U \gets getStart(G)$ //random start city
\li $V \gets getCitiesExceptStart(G)$
\li\While $V$ not empty:
\li $u \gets$ most recently added vertex to $U$
\li Find vertex $v \in V$ with smallest cost $u$
\li $U.add(v)$
\li $V.remove(v)$\End
\li $findRoute(U, getStart(G))$
\li\Return $U$
\end{codebox}
And below the Java equvilent:\\
$\proc{public ArrayList$<$String$>$ solve}()$ throws NoSolutionException\{
\begin{codebox}
\tab//Initializing variables\\
\tab long startTime = System.nanoTime();\\
\tab long seed = System.nanoTime();\\
\tab Collections.shuffle(V,new Random(seed));\\
\tab U.add(V.get(0));//random start\\
\tab V.remove(0); double distance = 0.0;\\
\tab\While(!V.isEmpty())\{//The algorithm\\
\tab\tab String u = U.get(U.size()-1);\\
\tab\tab Edge e = findMinVertex(graph.getCity(u),graph);\\
\tab\tab graph.setRoadVisited(true,e);\\
\end{codebox}
\begin{codebox}
\tab\tab String nextCity = e.getTo();\\
\tab\tab distance += e.getDistance();\\
\tab\tab U.add(nextCity); V.remove(nextCity);\\
\tab\}\\\End
\tab Edge road = findRouteToStart(U);\\
\tab U.add(road.getTo());\\
\end{codebox}
\begin{codebox}
\tab distance += road.getDistance();\\
\tab U = getMetrics(U,startTime,distance);\\
\tab \Return U;\\
\}
\end{codebox}
As the runtime of this algorithm is $O(n^2)$\cite{htspc} with a space complexity of $O(n)$ and in most cases though it will be within $25\%$ of the Held-Karp lower bound\cite{htspc}. The obvious issue is that due to the greedy local property of this heuristic, the final edge that could be used could be arbitrarily long. Due to this there is not a guarentee for there to be a upper bound that is a multiple of a constant $k$ of the optimal route. This is not optimal in terms of the distances the algorithm produces.\\
\tab As you can see the implementation of Nearest Neighbor is relatively simple and only requires a few additional helper functions including findMinVertex() and\\ findRouteToStart(). For the purposes of focusing on the algorithm these functions are not shown above. The thing to also notice is that the only additional knowledge required for this implementation (aside from Java) is the use of a loop and some minor intuition. With this in mind it is nice to have a simple implementation of such an algorithm that can find some approximate solution very quickly. An additional part to keep in mind is that depending on the starting city for the tour (which is random) for the simplicity and testing purposes of the implementation, it is possible for several different routes to be produced. With a few simple tweaks the implementation could be altered to accommodate a specified starting city. \\
\tab It should be noted in the data (to be provided) that the actual route and distances vary in both terms of the cities to be visited and in terms of the distance that it takes to travel. This while not optimal in terms of having a reliable answer is not necessarily an impairment to the approach as a whole. The reason for this logic is that the algorithm could be run multiple times (as is the case of testing) and keep track of the most efficient route found thus far, however in terms of the algorithm itself its benefits are that it is the simplest algorithm in terms of the concepts needed for the implementation as well as the amount of code for the implementation itself.

\section{Greedy}
\tab The second tour construction heuristic algorithm that is being examined is also a greedy approach similar to the Nearest Neighbor algorithm but is called the Greedy heuristic. As the name suggests the Greedy heuristic uses a Greedy approach to constructing the tour of the traveling salesman. The idea is simple and intuitive, simply sort all of the edges and use the shortest edge to construct the tour as long as you don't raise the degree of the vertexes more than two and that the edge doesn't make a cycle of less than $N$ where $N$ is the number of vertexes in the graph. The pseudo-code for the algorithm is displayed below.\\
$\proc{GreedyHeuristic}(G)$:
\begin{codebox}
\li $E \gets G.getRoads()$
\li $sort(E)$
\li $R \gets []$
\li $N \gets$ number of Vertices in G
\li\While $R.size <= N$:
\li $r \gets E[0]$
\li \If r does not makeCycle(G, r) and does not raiseDegreeMoreThan2(G, r)
\li R.add(r) \End 
\li remove r from E \End
\li\Return findRoute(R)
\end{codebox}\
The Java implementation:\\
$\proc{public ArrayList$<String>$ solve}()$ throws NoSolutionException\{
\begin{codebox}
\tab//Initializing variables\\
\tab long startTime = System.nanoTime();\\	
\tab ArrayList<String> result = new ArrayList<String>();\\
\tab//sorting and ensuring there are no duplicates\\
\tab Collections.sort(roads);\\
\tab maxEdges = graph.getCities().keySet().size();\\
\tab route = new ArrayList<Edge>();\\
\tab//finding the route\\
\end{codebox}
\begin{codebox}
\tab int numEdges = 0;\\
\tab\While(numEdges $<$ maxEdges)\{\\
\tab\tab Edge road = roads.get(0);\\
\tab\tab \If(!moreThanTwoDegrees(road,route)\\
\tab\tab\tab \&\& !makesCycleLessThanN(road, route))\{\\
\tab\tab\tab pathGraph.addEdge(road);\\
\tab\tab\tab route.add(road);\\
\tab\tab\tab numEdges++;\\
\tab\tab\}\\\End
\tab\tab roads.remove(0);\\
\tab\}\\\End
\tab //obtaining the result\\
\tab double distance = findRoute(result, route);\\
\tab //final preparing of the data\\
\tab result = getMetrics(result,startTime,distance);\\
\tab \Return result;\\
\}
\end{codebox}
\tab As can be seen above this is also a very intuitive construction heuristic based of the theory that using the shortest edges will help create the shortest route. However, like Nearest Neighbor, this algorithm has no guarantee of a upper bound on the distance returned though the Greedy algorithm averages within 15-20\% of the Held-Karp lower bound\cite{htspc}.\\
\tab Before moving forward with the analysis of this solution, it should be noted that there are two different graphs here. The first (named graph) is similar to the Nearest-Neighbor approach and is simply is the graph that the solver is working with. The second graph (named pathGraph) is just the vertexes of graph and is used to help simplify the examination of the edges added to the graph in the form of the functions makesCycleLessThanN() and moreThanTwoDegrees(). These functions each take $O(n)$ time in the worst case scenario. The check for making a cycle is simply an iterative linear-search since the graph used (the pathGraph) only has the set of roads in the route making checking for a cycle simply going from one node to the next and seeing if we have seen it before. The process of checking if an edge would increase the degree to be greater than two also relies on the path graph and count the instances that a edge is being used to go to a node and from a node and if the sum of any of counts for a city are more than 2 then returning true. This means that the overall runtime of the algorithm is at most $O(N^2)$ where E is the number of Edges that the graph has. Now I say this in the instance that there is no guarantee that in the number of cities N a solution will be found but in iterating over the Edges we find the solution doing $O(N)$ work from the previously mentioned functions for each Edge. It should be noted however, that the sorting of the edges is done int $O(nlogn)$ time and thus in this case is $O(ElogE)$ time and $O(E)$ space used. Therefore the entire algorithm runs in $O(ElogE + N^2)$ or $O(N^2)$ time and $O(E+N)$ space.\\
\tab From this approach a few things are especially reverent, the first is that since we are looking at the same set of sorted roads each time the solver is run, while the start of the route may change, the amount of time, distance and space stays relatively static. This provides for a consistency guarantee that might be desirable in some instances. The amount of overhead knowledge that is required by this algorithm is also very small, all that is required for the helper functions is some knowledge of graph theory with some simple algorithms related to finding cycles. If you are also constructing the actual graph (as was done above) then this task become even simpler to accomplish. Overall, this is a simple algorithm to also implement but it should be no surprise that the developer would need to not only have the Edges implement a comparable interface but also ensure that the destructive behaviors of the algorithm do not affect the graph as a whole.

\section{Minimum Spanning Tree}
\tab The final construction heuristic that this paper examines is the Minimum Spanning Tree heuristic. The basic approach is to using the graph make a minimum spanning tree and then create the tour from a preorder transversal of the tree. For some explanation, a minimum spanning tree is an acyclic graph connecting all of the vertexes with each of the vertexes having a degree no more than two. This is also an unintuitive algorithm since for most know do not study computer science, they would not necessarily think of creating a tree to make a cycle. The deceptively simple pseudo-code for the approach is below:\\

$\proc{MSTHeuristic}(G)$:
\begin{codebox}
\li T $\gets$ make a MST of G
\li \Return the depth-first transversal of T
\end{codebox}
This is a little more complicated in practice as seen in the Java Implementation:\\
$\proc{public ArrayList$<String>$ solve}()$ throws NoSolutionException \{
\begin{codebox}
\tab long startTime = System.nanoTime();\\
\tab ArrayList$<$String$>$ V = new ArrayList<String>();\\
\tab \For(String v: graph.getCities().keySet())\\
\tab\tab V.add(v);\End\\
\tab long seed = System.nanoTime();\\
\tab Collections.shuffle(V,new Random(seed));//random start\\
\tab//make the Tree\\
\tab Tree mst = makeMST(V.get(0));\\\\
\tab//get the result\\
\tab ArrayList$<$String$>$ result = mst.treeWalk();\\\\	
\tab//get the metrics\\
\tab result.add(result.get(0));\\
\tab double distance = getRouteDistance(result);\\
\tab result = getMetrics(result,startTime,distance);\\
\Return result;\\
\}
\end{codebox}
\tab As stated before, this is not an intuitive heuristic; this approach involves a couple of more complex algorithms than in previous approaches. First of all this involves the makeMST() function. For the purposes of this paper, the algorithm used was based off of Primm's MST algorithm. Primm's MST algorithm is a greedy algorithm, and with the implementation of this algorithm, there is a slight difference with the original algorithm. The original algorithm uses a data structure called a heap used as well as an adjacency list for the graph so it runs in $O(ElogN)$ time, but the implementation that is used collects the edges on the frontier and sorts them. While this takes $O(NElogE)$ time, it is a slightly simpler implementation then the use of a heap and exacting the values no longer applicable from it when the frontier changes. The time $O(NElogE)$ also is the upper bound time for this algorithm since the transversal of the tree takes $O(N)$ time since an iterative preorder to save space on the virtual stack uses an iterative traversal.\\
\tab The benefits to this heuristic is that there is a guarantee of a upper bound on the distance that the route produces of 2 times the optimal route. This is a nice change from the other heuristics that might have better run times because the other algorithms explained thus far do not have a guarantee making them potentially impractical for real time use. The second benefit is that if there are data structures and functions already in place that can be used such as a Tree, the function to make the MST, and the tree transversal function, then the actual code necessary is unbelievably straight forward (as the pseudo-code demonstrated). However as was said before there were a fair number of additional classes that were created so that this function was possible which make the implementation more complex than the previous heuristics.

\section{Genetic Algorithms}
\tab The only tour optimization heuristic that this paper examines is the popular family of algorithms called genetic algorithms. These algorithms mimic the evolutionary process by having a population, a fitness characteristic, a breeding mechanism, and a chance at mutation within individuals. The basic concept is to implement each of the characteristics and at the end of a number of iterations pick the fittest example as the result. This while not the most intuitive algorithm also makes a measure of sense if one thinks about it briefly. The psuedo-code for the algorithm helps show this approach below:\\
$\proc{GeneticHueristic}(G)$:
\begin{codebox}
\li V $\gets$ G.vertices()
\li population $\gets$ several random orderings of V
\li \For $k$ times:
\li find the best canidates from population
\li breed those candates
\li randomly mutate some of the offspring \End
\li \Return the most fit member of the population
\end{codebox}

And the Java implementation below:\\
$\proc{public ArrayList$<String>$ solve}()$\{
\begin{codebox}
long startTime = System.nanoTime();\\
\tab ArrayLis$<$ArrayList<String$>>$ population =  makePopulation();\\
\tab\For(int k = 0; $k < 500;$ k++)\{\\
\tab\tab ArrayList$<$ArrayList<String$>>$ parents = darwinsTheory(population);\\
\tab\tab population = breed(parents, population.size());\\
\tab\tab population =  mutate(population);\End\\
\tab\}\\
\tab ArrayList$<$String$>$ result = findBest(population);\\
\tab double distance = getRouteDistance(result);\\
\tab result = getMetrics(result,startTime,distance);\\
\Return result;\\
\}\\
\end{codebox}
\tab This is a tour improvement heuristic focusing on trying to find the best route by combining aspects of the shrotest routes. This is clearly not a singularly definitive approach to this problem. To clarify some of the finer points of the of the implementation. The first is that the initial population is simply a list of randomly made routes with the restriction that there are no two routes in the orginal population. This is to better increase the chances that the heuristic will produce a good route. The second thing to focus on is the breeding fuction. What is implemented is called a moon breeding. With this approach, basically $2/3^{rds}$ of the first parent is used and then the rest is filled in with the second parent. This is done for every pairing of the best canidates before mutations can take place. The number of mutations and chance of mutation are also both random, alloing for at least 1 mutaiton to potentially take place.\\
\tab In looking at this approach it is clear that there is a good chance for a good route to come out of this hueristic. The main issue lies in the fact that there is no guarentee of a decent result coming from this approach. In addition, the time it takes to run this heuristics is $O(kN^2)$ where k is a non-negliable constant. In short this hueristic has the potential to preform well but also does not make any guarentees of it's preformance making it a good appoach if you have a series of decent routes but not by any means optimal.

\section{Christofides}
\tab One of the most complicated heuristic algorithms is the famous Christofides heuristic. The approach of this hueristic builds off the concepts of the MST heuristic by first building a MST. Then second find all of the nodes with odd degrees and create a set of minimum pairs with those nodes. Then create a new graph that is the union between the MST and the new set of minimum pairs. To find the route to take then find the Euler Cycle (a path that visits each of the edges once) of the new graph and then take shortcuts if we have visited the node before. The basic pseudo-code for the algorithm described above is shown below:\\
$\proc{ChristofidesHeuristic}(G)$:
\begin{codebox}
\li $MST \gets makeMST(G)$ //random start city
\li $oddNodes \gets MST.findOddNodes()$
\li $MinPairs \gets findMinPairs(oddNodes)$
\li $G' \gets UNION(MST,MinPairs)$
\li $cycle \gets findEulerCycle(G')$
\li\Return $takeShortCuts(cycle)$
\end{codebox}
and the Java Implementation below:\\
$\proc{public ArrayList$<String>$ solve}()$\{
\begin{codebox}
\tab long startTime = System.nanoTime();\\
\tab ArrayList$<String>$ V = new ArrayList<String>();\\
\tab \For(String v: graph.getCities().keySet()) \\
\tab\tab V.add(v);\\
\tab long seed = System.nanoTime();\\
\tab Collections.shuffle(V,new Random(seed));//random start\\
\tab //make the Tree\\
\tab Tree mst = makeMST(V.get(0));\\
\tab ArrayList$<String>$ oddNodes = mst.getOddNodes();\\
\tab ArrayList$<Pair<String,String>>$ pairs = getMinimalPairs(oddNodes);\\
\tab eulerGraph = unionWithTree(pairs, eulerGraph);\\
\tab ArrayList$<String>$ cycle =  eurlerCycle(eulerGraph, V.get(0));\\
\tab ArrayList$<String>$ result = takeShortcuts(cycle);\\
\end{codebox}
\begin{codebox}
\tab double distance = getRouteDistance(result);\\
\tab result = getMetrics(result, startTime, distance);\\
\tab \Return result;\\
\}\\
\end{codebox}
\tab As one can see here, the basic layout of the heuristic is relatively straightforward. The complexities arise however upon the inspection of the helper methods. The various methods needed include finding the Minimum Spanning Tree (with the added part of adding the edges of the Tree to a graph), finding the nodes of odd degree, finding the minimum pairs, finding the Euler Cycle, and taking shortcuts to find the final route. As a result of these methods especially the findMinimumPairs the runtime of Christofides heuristic is $O(N^3)$. The space taken up by this heuristic is similar to the others in that it takes O(N) space to hold the MST and new graph. \\
\tab However, with this trade off of extra time, there is a guaranteed upper bound of any route produced by Christofides to be at most 3/2 times optimal route. This is better than the MST heuristic that has an upper bound of 2 times the optimal length and the other heuristics mentioned that don't have a upper bound. All of that being said, the complexity of implementing this algorithm is non-trivial and neither is the knowledge required to implement it. The complexity comes from the use of several extra functions needed to  complete the heuristic, each one could be a potential point of failure in the implementation. The second aspect required the might be a limitation for implementing this heuristic is the knowledge required to implement or understand it. The concepts needed include minimum spanning trees, minimum pairs, and finding Euler cycles in a graph. While these are not insurmountable levels of knowledge it does take more time for one to be familiar with the concepts before implementing and thus takes more time to write.

\chapter{Possible Improvements}
\tab With all of the heuristics examined it should be noted that given the time constraints of this paper, there are some improvements that could be desired. Some of these include the implementation of some of the helper functions and other's address more of the complexities for implementation in general. The set of possible improvements will then be addressed in a few categories: overall changes, alternative helper functions/implementations, and the complexities of implementation. The overall changes that are going to be described include the placement for the declaration of variables, some common trends that are prevelent in the implementation of the heuristics, and some of the questions surrounding the implementation. The alternative helper functions goes into detail about the various implementations that were used and the various alternatives that could have been implemented instead. Finally, the difficulties in implementation goes over all of the various difficulties and choices that were faced in the course. These categories reflect the challenges that one would face in implementing the heuristics described and provides for potential future work in the study of implementing TSP heuristics.\\
\section{General Improvements}
\tab Before delving into the details of what could be improved first a note on the overall implementation/practices that were used or could be improved on. First, as a note the code that was produced during the course of this paper while it attempts to implement the best practices for reusable code, is not perfect. First of all, the algorithms were written for both ease of implementation and for the accuracy of the result. This means that while the implmentations may not be the most efficient they do maintain a high level of accuracy. As such, the practices of declaring variables within loops instead of out of, not using built in iterators, or the creation of new data objects are all examples of sacraficing space for the sake of ease of implementation. Another point that should be addressed is that the implementations are all in Java which has the ability to be run on any platform but is not nessecarily the most efficient. In short, some things to keep in mind are that there is the use of new objects to ensure that accuracy and speed of implmentation and that there are minor improvements that could have also been made in addition to large algorithmic or implementation changes.\\
\tab There are also other forms of representing a graph that could be used instead of the matrix model that was mainly used. Other forms could be either using list of the edges going from a node and the use of actual node edge structure. Another item to note is the representation of the Edges and the Nodes in the graphs themselves. As their own classes, the Edges and Nodes mainly contained getting and setting functions and were primarily implemented to allow for some easier access to the meta-data and for flexibility in graph implementations. It is powerful to be able to look at a edge and to see the points that the edge goes to and from for instance. The edges are also directed, making a fair number of the algorithms more compleicated. This directed nature was countered by simply using two edges to `'represent" a singular edge and the algorithms that require undirected graphs took advantage of this property. This issue arose from the use of a class to represent an edge and by the constraints to make the class consistant. If one were to optimize this they might use mreo traditional methods for representing graphs without the use of classes for the nodes and edges. Finally, as a trend in the implemetation of the heuristics is the extensive use of HashMaps. This is not nessarily a problem with the implementation but it does mean that it is the only way to implementation.\\
\tab For several of the heuristics there are helper functions that were crucial to the implementation of the heuristic, for instance the MST algorithm for the Christofides and MST heuristics. There are obviously several different algorithms that could have been implemented but in the interest of time only one was implemented. Here the specific questions that could arise from examining the code for this paper will be addressed in the order that they were discussed in section three. As a common thread for the need for improvements it is safe to assume that most improvements could and would be made in an environment with more people working as a team and a singualar approach chosen instead of five. Obviously as the more complex heuristics are examined there are more questions that have to be addressed. For the sake of sticking to the point the key functions or concerns about the approaches will be examined and not nessecarily every detail of the implementation.\\
\section{Nearest Neighbor and Greedy}
\tab For the Nearest Neighbor and Greedy heuristics there are not a lot of specific improvements. These improvements include the use of declaring variables outside of the loops for instance. Also it is worth it to note that the classes such as the edges a verticies to help ease the difficulty of implementation. It  For the Greedy heuristic the choice to use an extra graph for the sake of simplifying the algorithm for checking a cycle. This choice means that there is a slight increase in space that is needed though overall space complexity of the algorithm.Typically, the checking for a cycle in a graph uses a breadth-first search instead of the implementation which given the knowledge of the graph to be searched used a depth-first search. This choice was made to simplify the overall implementation and draws on the choice to use a second graph in the Greedy heuristic. The second item that could be improved is when counting the degree of the nodes in the graph. The implementation that is used relies extensively on the use of HashMaps instead of checking the each of the nodes in the graph and counting the edges. For the purposes of making improvements to the approaches there are only a few optimizations in general that could be implemented to help the efficiency of the heuristics discussed in the overall comments on implementations. It is simple to see that the heuristics do not nessesarily have for lack of better terms less moving part and the number of improvements mimiced it.\\
\section{Minimum Spanning Tree}
\tab Naturally, the third heuristic that needs to be examined is the MST hueristic and the two different algorithms that could be used for the construction of the MST. There are two different algorithms that could have been used Primm's algorithm and Kruskal's algorithm. The difference between the two obviously come from the implementation and the subsequent runtimes of the algorithms. A varient of the Primm's algorithm was used for the purposes of this paper for the ease of implementation. This hueristic could simply be improved on by the use of a priority queue (or min-heap) instead of a constant use of an ArrayList that is constantly being sorted. The use of a priority queue allows for the sorting to happen during the insertion and removal instead of explicity sorting at each step. The use of Kruskal's algorithm could also improve the runtime but means that a whole new data structure would have needed to be created to fufilled the need of the algorithm.\\
\tab After examining the creation of the MST it us only natural to examine the use of the transversal for the MST heuristic. This comes because the lack of specificity in the description of the heuristic's approach. The approach only specifices that the heuristic uses a depth-first tranversal. This leads to three different approaches on how to implement the heurstic: in-order, pre-order, and post-order. After some experiementing witht he various tranversals, preorder was determined to be the most efficent since it arguably better maintains the relations between the various cities as they are added to the route in the creation of the the MST. In-order transversals work well with the exception that the various edgesan were added to the MST in the order of the shortest to the longest while a in-order transversal may not maintain this property. Finally, a post-order transversal could also maintain the property in the creation of the MST however was not implemented for the sake of readablity. Finally, these transversals could have been written iteraitvely or recursively but for the sake of not causing stack overflow a iterative approach was used instead of the recursive approach. These choices for the MST heuristic could be examined in more detail in a different paper.\\
\section{Genetic Algorithms}
\tab To cover all of the possible varients of the Genetic algorithm that could have been implemented is well outside the scope of this paper unfortunately. What can be explained in further detail though is the logic following the approach that was implemented in the course of this paper. First the initial population as and unique is an attempt at starting with a population with a greater chance of good parents coming from the initial population. Of course due to the non-deterministic nature of this start there is no gaurentee of a good result coming from this. The mutations are also are not only randomly enacted on the individual and there are a random number of mutations (at least one though) to allow for more variablity in the children. Finally, of the breeding of the individuals is a key part of the heuristic. There are several approaches ranging from the exact copying of the parents to the alternating of the cities in the parents for the children. The reason for choosing the moon breeding approach is to attempt to gather the best parts fo each of the parents. This is not by any means the most efficient nor can be described as only approach that can be explored more fully elsewhere.\\
\section{Christofides}
\tab Finally, the Christofides heuristic is obviously the most complex heuristic examined in this paper and thus has some of the most options for improvements. Some of these improvements include the use of various minimum spanning tree algorithms that have been examined in other heuristics. Some of these issues included the implementation of finding a euler tour, the taking the shortcuts to find the result, the union of the tree and minimal pairs, and the finding the minimal paris themselves. Before moving forward, as in the case of the Greedy heuristic, the Christofides heuristic uses an additional graph to store all of the edges in the Euler graph to be transversed. This choice came from logic that it is easier and more time cost efficient to not try and filter the original graph but to use it for the various edges needed to create the graph to be transversed. This is not the only method for creating the graph, other possible implementations including the marking edges in a certain way or eliminating all other edges just to name a few.\\
\tab The algorithm used for finding the euler cycle is a standard iterative stack implementation, the alternative is to use a recursive approach. The recursive approach would have taken less space than it's iterative counterpart but also runs the risk of creating a stack overflow error and as such was not used given the potentially large number of cities (and edges) a given gaph would contain. For taking the shortcuts it could be argued that an effective method would be to go find the result is to simply check to see if a city is already in the route before adding it to the final route. This mast sense intunitively but algorithmicly means that there is an extra factor of N withing the runtime of the algorithm. For the union of the tree, traditionally what should happen is the Tree and the pairs form a new graph created at the moment of the union. However, to speed up this process and to simplify it the extra graph was created with the additional edges of the MST being added at the time the tree was created. This meant that for the union all that needed to be added was the pairs to the extra graph. This is another example of the sacrafice of space for the benifit of time and simplicity. If one were to use the alogrithm as it was orignally written, then changes would need to be made to allow for a more visible tree and pairs.\\
\tab Finally, the creation of the Minimal Pairs is also not exactly as the algorithm described. This is due to the constrant that the underlying graph structure uses to contain the edges: a weighted matrix. The issue with the weighted matrix is that the graph cannot contian multiple edges from one city to another (ie there is only one edge AB instead of potentially more). This could be changed by simply using a list representation of the graph or even the original structure of the graph with nodes and links. The overall result for the finding of minimal pairs is still roughly the same regardless but just requires extra code and time to find the pairs (though not enough to change the overall runtime).\\
\section{Commentary on Implementation}
\tab As some more notes for the improvements that could have been made, there is commentary on the implementations of the various heuristics that should be addressed. The first is that within the various algorithms and approaches that are used in the heuristics there ar additional functions and methods that the implementor has no control over in using one language over another. For instance the contains function that is key to ensuring that there are not any null-pointer exceptions or other errors that could cause the halting of the program however it takes O(N) time to execute in Java. Other such functions also depend on the datastructures that were used for the implementation of the heuristics such as ArrayLists and HashMaps. These containers can be used in their myriad of forms ranging from the simple linked list to the size changing vectors (e.g. ArrayList) or even using arrays of immutatble size. Each of the containers has a specific role and futher examination of the efficiency of these containers on the heuristics could be examined in later research. For reference the extensive use of ArrayLists and HashMaps does not reflect a deep algorithm choice but rather the pragmatic choice of understanding the mutatibility of the structures and the uncertainty of the size and compositions of graphs. However, in using these built in data structures there are also some risks of `'hidden" complexities to be added to the overall complexity. These `'hidden" complexities range from tha aformentioned contains fuction to the get and put functions of the HashMaps or even to the insertion and removal of objects in a ArrayList. with this in mind it is important to note that there is room for improvements in the implementation but not all of the inefficiencies are caused by the impementation but rather overhead.\\
\tab The final comments on implementation should be on the langugage used for the entire paper, Java. The obvious issues with Java include the fact that the Java is run on a virtutal machine instead of the computer itself. This abstraction is useful for allowing the hueristics to be run on multiple platforms but not in terms of speed. The speed of the JVM is the limiting factor creating more overhead in terms of the objects needed for the actual running of the heuristics. Also, given the object oriented nature of Java, the use of object oriented practices, while useful for more reusable code also means that the need for more space is required. To improve on this, there could be investigation into the various programming languages and paradigms on the implementaions of the heuristics and the runtimes and space usage. However, such questions are beyond the scope of this paper but reguardless are good questions and ideas to keep in mind for potential future work and study.\\
\tab Overall, the improvements that could have been implemented instead of the implementations that are used in the course of this paper. The improvements would include the use of different algorithms in the heuristics, better coding practices in general, and even other languages. However, most of the improvements re outside of the scope of the paper in terms of time and content. With the discussion of improvements acknowledged, the next logical step is to discuss the actual results of the hueristics.

\chapter{Results}
\tab It is clear from the previous sections that there is a level of complexity in each of the implementations there is a level of algorithmic complexity and implementation complexity that should be considered. This leaves the question however of how does this translate in to quantifiable comparisons between the different heuristics? To help with examining the overal preformance of the heuristics several charts have been displayed and each one will be explained in detail. Each of these charts comes from running the heuristics on five different states with a different numbers of cities for each. The charts that will be examined are the bar-and-whisker (box) plot and several line charts. The bar and whisker plots for those that do not knwo what they are or hwo to read them look at not only the average of the data gathered but also the distribution of that data. The line charts represent the scaling of the heuristic's time multiplied by the distance over the number of cities as well as the space consumption multiplied by the distance. These measurements are meant to show the overall efficiency of the heuristics in terms of the time and the space and how those heuristics compare overall. The final set of charts goes over the amount of time that the heuristic uses by the distance so that as potential implementers, there can b an idea of what the expected distances for a given heuristic would be.\\
\tab To confirm the differences in the data there were t-tests run on each of the metrics (distance, time and space). Each of these t-tests were taken in relation from one heuristic to the others and had values of below $.29$ if not significantly less. This helps show that the data from each of the heuristics is different enough to suggest a strong difference and validity in the results.\\
\tab Below in figure 5:1 there is bar and whisker plots with and without the genetic algorithm respectively. Each of these graphs shows the distribution in the distances the routes provide interms  of the number of cities they were given.  From what can be seen below: it is clear that overall, the heuristics come up with similar distances and distributions with a few outliers to be noted. The first is that the genetic algorithm was not preforming very well or consistantly and the second is that due to the determinism of the Greedy heuristics there is no distribution though it is hardly reliable since there are some cases where it will not find a route (in this case the state of Delaware with 83 cities). Before moving on to other heuristics it is importnats to note that there is no correlation between the number of cities and the distances of the routes found. This is reflected in the fact taht there can be more cities densely packed together shortening the network distances of the edges.\\
\begin{figure}[t]
	\begin{center}
	\caption{The top plot has all of the heuristics with the botthom all but the Genetic hueristic.}
	\includegraphics[width=.95\textwidth]{box_w_gen}
	\includegraphics[width=.95\textwidth]{box_wo_gen}
	\end{center}
\end{figure}
\tab Upon inspecting these graphs it is clear to see that the algorithm that consistantly finds the shortest distance is the Christofides algorithm. This is to be expected given the upper bound guarentees of distance that the hueristic provides. As for the second best algorithm, there seems to be a toss up in terms of distances between the various other heuristics. The two more consistant algorithms were the Minimum Spanning Tree (MST) and the Greedy Heuristic. In terms of actutal average distance the Nearest Neighbor was comparable to both the MST and the Christofides heursitics. This suggests that the various approaches preform similarly and that the upper bound guarentees are a set of insurances but not a hardline for how the hueristic with preform in the real world. \\
\begin{figure}[t!]
	\begin{center}
	\caption{The top plot has all of the heuristics and the their runtime  and the bottom shows has all but their memory use.}
	\includegraphics[width=.95\textwidth]{time_numcities}
	\includegraphics[width=.95\textwidth]{mem_numcities}
	\end{center}
\end{figure}
\tab Needless to stay ther are more comparisions that need to be made to see how these heuristics preformed overall. The next set of graphs is based on the usage of the memory (in Kilobytes KB) and the time (in seconds) of each of the heuristics. For the sake of simplicity the mean average for the time and space measurement have been plotted on figure 5:2.\\
\tab For the amount of time that the heuristics run in there are a few intersting items to be observed. First is that Nearest Neighbor heuristic consitantly has a runtime of near zero and stays close to that regardless of the number of cities. This is parculiar since it means that with a single pass through the data that it finds a solution instead of having to do several extra functions at each iteration. As for the next two heuristics MST and Christofides, there is clearly more time used and this can be described two ways. The first is that the algorithmic complexities are higher than the NN aproach by a factor of $\log(n)$. While this does not seem like a lot it clearly has an impact on the actual runtime of the heuritics. With the Genetic heuristic then also took a lot of time. This comes from the constant amount of iterations that heuristis always runs reguardless of if the heuristic as found a good solution or not. While in the long run the other heuristics should take more time than the offset of the constant but apparently for the states that were used in the test, this is not the case. This is interesting in that it means that there could be room for improving the stopping point of the Genetic hueristic. Finally, the other item that should be observed is the amount of time that it takes for the Greedy heuristic to run. The interest stems from the fact that the algorithmic complexity is $O(N^2)$ just like the Nearest Neighbor heuristic but takes vastly more time than Nearest Neighbor. This could be because the heuristic has to take more iterations to find the solution and for iteration checking the degrees and if a cycle is made. The fact that there are two steps in each iteration allows for and extra amount of work that was not present in the other constuction heuristics. It also is telling that the approach apparently expends the entirety of the number of iterations to find the solution hence the running times of the heuristics.\\
\begin{figure}[h!]
	\begin{center}
	\caption{The top plot has all of the heuristics and the their runtime times the distance and the bottom shows has all but their memory use times their distance.}
	\includegraphics[width=.95\textwidth]{timexdistance_numcities}
	\includegraphics[width=.95\textwidth]{memxdistance_numcities}
	\end{center}
\end{figure}
\tab In regards to the memory usage the methods used were described in chapter 2, however the spikes in Nearest Neighbor and MST are supposedly due to the startup memory that is need at start up in running the heuristics. What is interesting here though is that the algorithmic space complexity is $O(N+E)$ or $O(N)$ for each of the heuristics and that the usage of memory conforms to this. This is seen through the slight parabolic curves of the the heuristics. The non linear curve could easily be explained by the amount of memory that the graph takes up for each heuristic $O(N^2)$. The consistant slope of each of these heuristics is due to the fact that the hueristics are all adding a similar amount to the memory usage as a whole. As for the early spikes in the Nearest Neighbor heuristic and the Minimum Spanning Tree heuristic, it can only be supposed that this phenomeon is due to the overhead of startup. That being said obviously there should be mroe investigation into this phenomenon, but in examining the individual characteristics it appears to be a single outlier that skews the whole mean. All and all, in terms of the memory usage all of the heuristics are similar enough to eachother in terms of average value that there is not a clear advangtage of one heuristic over the others.\\
\tab Figure 5:3 help bring the various heuristics into a sharper focus in terms of which are more useful in implementation than the others. This is attempted by looking at in essence the ratio of the distace and the time and memory used for each heuristic. The result is that thier is a greater separation of the heuritics than in the previous figures. The result of thsi separation allows for a more indepth inspection of the the various heuristics and their overall preformace across multiple metrics. For instance, while the genetic heuristic is in the relatively similar postion as in figure 5:1, the Greedy heusristic becomes distinct in looking at the product of time and distance. Interestingly, The other three heuristics are pretty similar to eachother but with Nearest Neighbor having the best ratio of distance and time. In terms of memory and distance, with the exception of genetic heuristics the heuristics are pretty similar. The difference in the gentic heuristic can be seen from the fact that the distances are on adverage significantly more than the others. Overall, it is clear that from an actual distance, runtime,a nd memory perspective that MST, Christofides, and Nearest Neighbor are the strongest heuristics.
\chapter{Conclusions}
\tab NN\\
\tab Greedy \\
\tab MST
\tab From just looking at the distribution of route distances and time and space usage, it is clear that the genetic algorithms are far from perfect and while they do almsot as well as the other heuristics on smaller numbers of cities are not the ideal approach. This is said based on the average distance and the distribution of distances, the ammount of time used which was the second worst among the heuristics and that the fact that the memor usage was on par with the results of the other heuristics.\\
\tab Christofides
\chapter{Acknowledgements}
Pending
\bibliography{Thesis}

\end{document}